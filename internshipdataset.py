# -*- coding: utf-8 -*-
"""InternshipDataSet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qVuUKaofzXpoFiOd2s3pDUnYIMS2KRxz
"""

from google.colab import files
import zipfile
import os


uploaded = files.upload()

Get the uploaded filename automatically ---
zip_path = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded file: {zip_path}")

#--- Unzip into a folder ---
extract_folder = "diabetes_csvs"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

# --- Verify extraction ---
print("Extraction complete")
print("Extracted files:", len(os.listdir(extract_folder)))


!pip install python-docx

import pandas as pd
import glob
import os
from docx import Document


path = "diabetes_csvs"
all_files = glob.glob(os.path.join(path, "*.csv"))

print(f"Found {len(all_files)} CSV files.")

# --- Create a Word document ---
doc = Document()
doc.add_heading("CSV Features Summary", 0)

# --- Extract features from each file ---
for file in all_files:
    try:
        df = pd.read_csv(file, nrows=5)   # Only first 5 rows for speed
        features = list(df.columns)

        
        doc.add_heading(f"{os.path.basename(file)} ({len(features)} features)", level=2)

        # Add features as bullet list
        for col in features:
            doc.add_paragraph(col, style="List Bullet")
    except Exception as e:
        doc.add_heading(os.path.basename(file), level=2)
        doc.add_paragraph(f"Could not read file: {e}", style="List Bullet")

# --- Save the Word document ---
output_doc = "features_summary.docx"
doc.save(output_doc)

print(f"Features saved to Word file: {output_doc}")



CSV_FOLDER_PATH = "/content/diabetes_csvs"  # Update if needed

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def create_diabetes_dataset(csv_folder_path):
    """
    Create a comprehensive diabetes prediction dataset by merging relevant NHANES files.

    Key files selected based on diabetes relevance:
    - Glucose measurements (GLU_*)
    - Insulin measurements (INS_*)
    - HbA1c (Glycohemoglobin) measurements (GHB_*)
    - Biochemistry profiles (BIOPRO_*)
    - Complete Blood Count (CBC_*)
    - Triglycerides and cholesterol (TRIGLY_*, HDL_*, TCHOL_*)
    - Blood pressure related (various)
    - Demographics and anthropometry would come from other NHANES files
    """

    # Initialize with empty DataFrame
    master_df = pd.DataFrame()

    # Define the most relevant files for diabetes prediction
    diabetes_relevant_files = {
        # Glucose and Diabetes markers
        'glucose': ['GLU_D.csv', 'GLU_E.csv', 'GLU_F.csv', 'GLU_G.csv', 'GLU_H.csv', 'GLU_I.csv', 'GLU_J.csv', 'GLU_L.csv'],
        'insulin': ['INS_H.csv', 'INS_I.csv', 'INS_J.csv', 'INS_L.csv'],
        'ghb': ['GHB_D.csv', 'GHB_E.csv', 'GHB_F.csv', 'GHB_G.csv', 'GHB_H.csv', 'GHB_I.csv', 'GHB_J.csv', 'GHB_L.csv'],

        # Lipid profile
        'triglycerides': ['TRIGLY_D.csv', 'TRIGLY_E.csv', 'TRIGLY_F.csv', 'TRIGLY_G.csv', 'TRIGLY_H.csv', 'TRIGLY_I.csv', 'TRIGLY_J.csv'],
        'hdl': ['HDL_D.csv', 'HDL_E.csv', 'HDL_F.csv', 'HDL_G.csv', 'HDL_H.csv', 'HDL_I.csv', 'HDL_J.csv', 'HDL_L.csv'],
        'tchol': ['TCHOL_D.csv', 'TCHOL_E.csv', 'TCHOL_F.csv', 'TCHOL_G.csv', 'TCHOL_H.csv', 'TCHOL_I.csv', 'TCHOL_J.csv', 'TCHOL_L.csv'],

        # Biochemistry profiles
        'biopro': ['BIOPRO_D.csv', 'BIOPRO_E.csv', 'BIOPRO_F.csv', 'BIOPRO_G.csv', 'BIOPRO_H.csv', 'BIOPRO_I.csv', 'BIOPRO_J.csv'],

        # Complete Blood Count
        'cbc': ['CBC_D.csv', 'CBC_E.csv', 'CBC_F.csv', 'CBC_G.csv', 'CBC_H.csv', 'CBC_I.csv', 'CBC_J.csv', 'CBC_L.csv'],

        # C-Reactive Protein (inflammation marker)
        'crp': ['CRP_D.csv', 'CRP_E.csv', 'CRP_F.csv', 'HSCRP_I.csv', 'HSCRP_J.csv', 'HSCRP_L.csv'],

        # Albumin/Creatinine ratio (kidney function)
        'alb_cr': ['ALB_CR_D.csv', 'ALB_CR_E.csv', 'ALB_CR_F.csv', 'ALB_CR_G.csv', 'ALB_CR_H.csv', 'ALB_CR_I.csv', 'ALB_CR_J.csv'],

        # Oral Glucose Tolerance Test
        'ogtt': ['OGTT_D.csv', 'OGTT_E.csv', 'OGTT_F.csv', 'OGTT_G.csv', 'OGTT_H.csv', 'OGTT_I.csv']
    }

    # Collect all unique files
    all_files = set()
    for file_list in diabetes_relevant_files.values():
        all_files.update(file_list)

    print(f"Processing {len(all_files)} relevant files for diabetes prediction...")

    # Process each file
    processed_files = []
    for filename in all_files:
        file_path = Path(csv_folder_path) / filename

        if file_path.exists():
            try:
                df = pd.read_csv(file_path)

                if 'SEQN' in df.columns and len(df) > 0:
                    print(f"‚úì Processing {filename}: {df.shape[0]} rows, {df.shape[1]} columns")

                    # Merge with master dataframe
                    if master_df.empty:
                        master_df = df.copy()
                    else:
                        # Merge on SEQN (participant ID)
                        master_df = pd.merge(master_df, df, on='SEQN', how='outer', suffixes=('', f'_{filename.split(".")[0]}'))

                    processed_files.append(filename)
                else:
                    print(f"‚ö† Skipping {filename}: No SEQN column or empty file")

            except Exception as e:
                print(f"‚úó Error processing {filename}: {str(e)}")
        else:
            print(f"‚úó File not found: {filename}")

    print(f"\nSuccessfully processed {len(processed_files)} files")
    print(f"Final dataset shape: {master_df.shape}")

    # Create diabetes target variable based on available glucose/HbA1c measurements
    master_df = create_diabetes_target(master_df)

    # Clean and prepare the dataset
    master_df = clean_dataset(master_df)

    return master_df, processed_files

def create_diabetes_target(df):
    """
    Create diabetes target variable based on standard diagnostic criteria:
    - Fasting glucose >= 126 mg/dL
    - Random glucose >= 200 mg/dL
    - HbA1c >= 6.5% (48 mmol/mol)
    """
    print("\nCreating diabetes target variable...")

    # Initialize diabetes status
    df['diabetes'] = 0
    df['diabetes_source'] = 'unknown'

    # Check for fasting glucose (LBXGLU columns)
    glucose_cols = [col for col in df.columns if 'LBXGLU' in col]
    fasting_cols = [col for col in df.columns if 'PHAFSTHR' in col]  # Fasting hours

    for glucose_col in glucose_cols:
        if glucose_col in df.columns:
            # Fasting glucose >= 126 mg/dL indicates diabetes
            fasting_diabetes = df[glucose_col] >= 126
            df.loc[fasting_diabetes, 'diabetes'] = 1
            df.loc[fasting_diabetes, 'diabetes_source'] = 'fasting_glucose'

            # Random glucose >= 200 mg/dL indicates diabetes
            random_diabetes = df[glucose_col] >= 200
            df.loc[random_diabetes, 'diabetes'] = 1
            df.loc[random_diabetes, 'diabetes_source'] = 'random_glucose'

    # Check for HbA1c (LBXGH columns - glycohemoglobin)
    hba1c_cols = [col for col in df.columns if 'LBXGH' in col]
    for hba1c_col in hba1c_cols:
        if hba1c_col in df.columns:
            # HbA1c >= 6.5% indicates diabetes
            hba1c_diabetes = df[hba1c_col] >= 6.5
            df.loc[hba1c_diabetes, 'diabetes'] = 1
            df.loc[hba1c_diabetes, 'diabetes_source'] = 'hba1c'

    # Check for OGTT (oral glucose tolerance test) results
    ogtt_cols = [col for col in df.columns if 'LBXGLT' in col]
    for ogtt_col in ogtt_cols:
        if ogtt_col in df.columns:
            # 2-hour glucose >= 200 mg/dL indicates diabetes
            ogtt_diabetes = df[ogtt_col] >= 200
            df.loc[ogtt_diabetes, 'diabetes'] = 1
            df.loc[ogtt_diabetes, 'diabetes_source'] = 'ogtt'

    diabetes_count = df['diabetes'].sum()
    total_with_data = df['diabetes_source'].ne('unknown').sum()

    print(f"Diabetes cases identified: {diabetes_count}")
    print(f"Total participants with glucose data: {total_with_data}")
    print(f"Diabetes prevalence: {diabetes_count/total_with_data*100:.1f}%" if total_with_data > 0 else "No glucose data available")

    return df

def clean_dataset(df):
    """Clean and prepare the dataset for machine learning"""
    print("\nCleaning dataset...")

    # Remove columns with all missing values
    initial_cols = df.shape[1]
    df = df.dropna(axis=1, how='all')
    print(f"Removed {initial_cols - df.shape[1]} completely empty columns")

    # Remove duplicate columns (keep first occurrence)
    df = df.loc[:, ~df.columns.duplicated()]

    # Focus on most relevant features for diabetes prediction
    diabetes_features = []

    # Key glucose/diabetes markers
    glucose_patterns = ['LBXGLU', 'LBXIN', 'LBXGH', 'LBXGLT']
    for pattern in glucose_patterns:
        diabetes_features.extend([col for col in df.columns if pattern in col])

    # Lipid profile
    lipid_patterns = ['LBXTR', 'LBDHDD', 'LBXTC', 'LBDLDL']
    for pattern in lipid_patterns:
        diabetes_features.extend([col for col in df.columns if pattern in col])

    # Key biochemistry markers
    biochem_patterns = ['LBXSGL', 'LBXSUA', 'LBXSCR', 'LBXSIR', 'LBXSPH']  # Glucose, uric acid, creatinine, iron, phosphorus
    for pattern in biochem_patterns:
        diabetes_features.extend([col for col in df.columns if pattern in col])

    # Blood count
    cbc_patterns = ['LBXWBC', 'LBXRBC', 'LBXHGB', 'LBXHCT', 'LBXPLT']
    for pattern in cbc_patterns:
        diabetes_features.extend([col for col in df.columns if pattern in col])

    # Inflammation markers
    inflam_patterns = ['LBXCRP', 'LBXHSCRP']
    for pattern in inflam_patterns:
        diabetes_features.extend([col for col in df.columns if pattern in col])

    # Kidney function
    kidney_patterns = ['URXUMA', 'URXUCR']
    for pattern in kidney_patterns:
        diabetes_features.extend([col for col in df.columns if pattern in col])

    # Add essential columns
    essential_cols = ['SEQN', 'diabetes', 'diabetes_source']
    diabetes_features.extend(essential_cols)

    # Remove duplicates and filter existing columns
    diabetes_features = list(set([col for col in diabetes_features if col in df.columns]))

    # Create final dataset
    final_df = df[diabetes_features].copy()

    print(f"Selected {len(diabetes_features)} relevant features")
    print(f"Final dataset shape: {final_df.shape}")

    # Remove rows where diabetes status is unknown
    final_df = final_df[final_df['diabetes_source'] != 'unknown']
    print(f"Dataset with known diabetes status: {final_df.shape}")

    return final_df

# Main execution
if __name__ == "__main__":
    # Set your CSV folder path here
    CSV_FOLDER_PATH = "/content/diabetes_csvs"  # Update this path as needed

    print("NHANES Diabetes Prediction Dataset Creator")
    print("=" * 50)

    # Create the dataset
    diabetes_df, processed_files = create_diabetes_dataset(CSV_FOLDER_PATH)

    # Save the dataset
    output_filename = "diabetes_prediction_dataset.csv"
    diabetes_df.to_csv(output_filename, index=False)

    print(f"\nüéâ Dataset saved as: {output_filename}")
    print(f"Shape: {diabetes_df.shape}")
    print(f"Diabetes cases: {diabetes_df['diabetes'].sum()}")
    print(f"Non-diabetes cases: {(diabetes_df['diabetes'] == 0).sum()}")

    # Display sample of the data
    print("\nFirst few rows of the dataset:")
    print(diabetes_df.head())

    print("\nColumn summary:")
    print(diabetes_df.info())

    print(f"\nProcessed files: {', '.join(processed_files[:10])}")
    if len(processed_files) > 10:
        print(f"... and {len(processed_files) - 10} more files")

import pandas as pd
import numpy as np
import warnings
import glob
import os

warnings.filterwarnings('ignore')

class DiabetesPredictionDataProcessor:
    def __init__(self):
        self.diabetes_relevant_datasets = {
            'GLU_*': ['SEQN', 'LBXGLU', 'LBDGLUSI', 'PHAFSTHR', 'PHAFSTMN'],
            'OGTT_*': ['SEQN', 'LBXGLT', 'LBDGLTSI', 'GTDSCMMN', 'GTDDR1MN', 'GTDBL2MN', 'GTDDR2MN'],
            'INS_*': ['SEQN', 'LBXIN', 'LBDINSI', 'LBDINLC'],
            'GHB_*': ['SEQN', 'LBXGH'],
            'TCHOL_*': ['SEQN', 'LBXTC', 'LBDTCSI'],
            'HDL_*': ['SEQN', 'LBDHDD', 'LBDHDDSI'],
            'TRIGLY_*': ['SEQN', 'LBXTR', 'LBDTRSI', 'LBDLDL', 'LBDLDLSI'],
            'BIOPRO_*': ['SEQN', 'LBXSAL', 'LBDSALSI', 'LBXSATSI', 'LBXSASSI', 'LBXSAPSI',
                         'LBXSBU', 'LBDSBUSI', 'LBXSCA', 'LBDSCASI', 'LBXSCH', 'LBDSCHSI',
                         'LBXSC3SI', 'LBXSCR', 'LBDSCRSI', 'LBXSGTSI', 'LBXSGL', 'LBDSGLSI',
                         'LBXSIR', 'LBDSIRSI', 'LBXSLDSI', 'LBXSPH', 'LBDSPHSI', 'LBXSTB',
                         'LBDSTBSI', 'LBXSTP', 'LBDSTPSI', 'LBXSTR', 'LBDSTRSI', 'LBXSUA', 'LBDSUASI'],
            'CBC_*': ['SEQN', 'LBXWBCSI', 'LBXLYPCT', 'LBXMOPCT', 'LBXNEPCT', 'LBXEOPCT',
                      'LBXBAPCT', 'LBDLYMNO', 'LBDMONO', 'LBDNENO', 'LBDEONO', 'LBDBANO',
                      'LBXRBCSI', 'LBXHGB', 'LBXHCT', 'LBXMCVSI', 'LBXMCHSI', 'LBXMC',
                      'LBXRDW', 'LBXPLTSI', 'LBXMPSI'],
            'FERTIN_*': ['SEQN', 'LBXFER', 'LBDFERSI'],
            'TFR_*': ['SEQN', 'LBXTFR', 'LBDTFRSI'],
            'HSCRP_*': ['SEQN', 'LBXHSCRP', 'LBDHRPLC'],
            'CRP_*': ['SEQN', 'LBXCRP'],
            'ALB_CR_*': ['SEQN', 'URXUMA', 'URXUMS', 'URXUCR', 'URXCRS', 'URDACT'],
            'THYROD_*': ['SEQN', 'LBXATG', 'LBXT3F', 'LBXT4F', 'LBDT4FSI', 'LBXTGN',
                         'LBDTGNSI', 'LBXTSH1', 'LBDTSH1S', 'LBXTPO', 'LBXTT3', 'LBDTT3SI', 'LBXTT4'],
            'VITB12_*': ['SEQN', 'LBXB12', 'LBDB12SI'],
            'FOLATE_*': ['SEQN', 'LBDRFO', 'LBDRFOSI'],
            'FOLFMS_*': ['SEQN', 'LBDFOTSI', 'LBDFOT', 'LBXSF1SI', 'LBDSF1LC', 'LBXSF2SI',
                         'LBDSF2LC', 'LBXSF3SI', 'LBDSF3LC', 'LBXSF4SI', 'LBDSF4LC',
                         'LBXSF5SI', 'LBDSF5LC', 'LBXSF6SI', 'LBDSF6LC'],
            'PBCD_*': ['SEQN', 'LBXBPB', 'LBDBPBSI', 'LBDBPBLC', 'LBXBCD', 'LBDBCDSI',
                       'LBDBCDLC', 'LBXTHG', 'LBDTHGSI', 'LBDTHGLC'],
            'VID_*': ['SEQN', 'LBXVIDMS', 'LBDVIDLC', 'LBXVD2MS', 'LBDVD2LC', 'LBXVD3MS',
                      'LBDVD3LC', 'LBXVE3MS', 'LBDVE3LC'],
            'FASTQX_*': ['SEQN', 'PHAFSTHR', 'PHAFSTMN', 'PHDSESN']
        }

    def load_and_process_datasets(self, data_path: str = '/content/diabetes_csvs/') -> pd.DataFrame:
        """Load and merge all relevant datasets into one DataFrame without duplicating columns."""
        print("üîç Loading and processing diabetes-relevant datasets...")

        all_dataframes = []

        for dataset_pattern, features in self.diabetes_relevant_datasets.items():
            if '*' in dataset_pattern:
                base_pattern = dataset_pattern.replace('*', '')
                search_pattern = os.path.join(data_path, f"{base_pattern}*.csv")
            else:
                search_pattern = os.path.join(data_path, f"{dataset_pattern}.csv")

            for file_path in glob.glob(search_pattern):
                try:
                    df = pd.read_csv(file_path)
                    available_features = [f for f in features if f in df.columns]
                    if len(available_features) > 1:
                        all_dataframes.append(df[available_features])
                        print(f"‚úÖ Loaded {os.path.basename(file_path)}: {len(df)} rows, {len(available_features)} features")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not load {file_path}: {e}")

        if not all_dataframes:
            return pd.DataFrame()  # no data loaded

        # Start with the first dataframe
        consolidated_data = all_dataframes[0].copy()

        # Merge remaining dataframes without adding new columns if they exist
        for df in all_dataframes[1:]:
            # Align columns: keep existing ones, add new ones as NaN if not present
            for col in df.columns:
                if col == 'SEQN':
                    continue
                if col in consolidated_data.columns:
                    consolidated_data[col] = consolidated_data[col].combine_first(df[col])
                else:
                    consolidated_data[col] = df[col]

            # Make sure all SEQN from new df are included
            consolidated_data = pd.merge(consolidated_data, df[['SEQN']], on='SEQN', how='outer')

        print(f"\nüìä Final merged dataset: {len(consolidated_data)} rows, {len(consolidated_data.columns)} features")
        return consolidated_data


def main():
    print("üöÄ Creating single CSV with relevant features only...")

    processor = DiabetesPredictionDataProcessor()
    final_df = processor.load_and_process_datasets('/content/diabetes_csvs/')

    if final_df.empty:
        print("‚ùå No data loaded.")
        return None

    output_file = '/content/diabetes_relevant_features.csv'
    final_df.to_csv(output_file, index=False)
    print(f"‚úÖ Final dataset saved as {output_file}")

    return final_df


if __name__ == "__main__":
    diabetes_data = main()

import pandas as pd
import warnings
import glob
import os

warnings.filterwarnings('ignore')

class DiabetesPredictionDataProcessor:
    def __init__(self):
        pass  # Take all columns, no pre-defined features

    def load_and_process_datasets_in_batches(self, data_path: str = '/content/diabetes_csvs/', batch_size: int = 100, batch_save_path: str = '/content/batches/') -> None:
        """Load datasets in batches, merge each batch on SEQN, keeping all unique columns, filling missing with NaN, and save batch CSVs."""
        print("üîç Loading and processing datasets in batches...")

        os.makedirs(batch_save_path, exist_ok=True)
        all_files = glob.glob(os.path.join(data_path, '*.csv'))
        if not all_files:
            print("‚ùå No CSV files found in the folder.")
            return

        # Process files in batches
        for i in range(0, len(all_files), batch_size):
            batch_files = all_files[i:i + batch_size]
            print(f"\nüì¶ Processing batch {i//batch_size + 1} ({len(batch_files)} files)")

            batch_consolidated = None

            for file_path in batch_files:
                try:
                    df = pd.read_csv(file_path)
                    if 'SEQN' not in df.columns:
                        print(f"‚ö†Ô∏è Skipping {os.path.basename(file_path)} (no SEQN column)")
                        continue

                    if batch_consolidated is None:
                        batch_consolidated = df.copy()
                    else:
                        # Merge on SEQN without creating duplicate columns
                        batch_consolidated = batch_consolidated.set_index('SEQN').combine_first(df.set_index('SEQN')).reset_index()

                    print(f"‚úÖ Loaded {os.path.basename(file_path)}: {len(df)} rows, {len(df.columns)} columns")

                except Exception as e:
                    print(f"‚ö†Ô∏è Could not load {file_path}: {e}")

            # Save the merged batch
            if batch_consolidated is not None:
                batch_file = os.path.join(batch_save_path, f'batch_{i//batch_size + 1}.csv')
                batch_consolidated.to_csv(batch_file, index=False)
                print(f"üíæ Saved merged batch as {batch_file}")

def main():
    print("üöÄ Processing all datasets in batches and saving merged batch files...")

    processor = DiabetesPredictionDataProcessor()
    processor.load_and_process_datasets_in_batches(
        data_path='/content/diabetes_csvs/',
        batch_size=100,
        batch_save_path='/content/batches_New/'
    )

if __name__ == "__main__":
    main()
