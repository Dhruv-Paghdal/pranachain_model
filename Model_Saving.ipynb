{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7c57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer, balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f03352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "df_336 = pd.read_csv(\"UCI_336_Augmented.csv\")\n",
    "df_857 = pd.read_csv(\"UCI_857_Augmented.csv\")\n",
    "df_ckd = pd.read_csv(\"CKD_Augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da4acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Harmonization and Feature Engineering ---\n",
    "# Target Unification and Feature Renaming for D3\n",
    "df_ckd = df_ckd.rename(columns={'Diagnosis': 'class', 'Age': 'age', 'SerumCreatinine': 'sc', \n",
    "                                'HemoglobinLevels': 'hemo', 'ProteinInUrine': 'al'})\n",
    "\n",
    "# Derived Proxy Features for D3\n",
    "df_ckd['htn'] = ((df_ckd['SystolicBP'] >= 140) | (df_ckd['DiastolicBP'] >= 90)).astype(int)\n",
    "df_ckd['dm'] = ((df_ckd['FastingBloodSugar'] >= 126) | (df_ckd['HbA1c'] >= 6.5)).astype(int)\n",
    "\n",
    "# Define Feature Sets\n",
    "features_m1 = ['age', 'htn', 'dm', 'cad', 'su', 'sg', 'al', 'bp', 'sc', 'hemo', 'rbcc']\n",
    "features_m2 = ['age', 'htn', 'dm', 'cad', 'su', 'sg', 'al', 'sc', 'gfr', 'hemo', 'rbcc']\n",
    "features_m3 = ['age', 'htn', 'dm', 'sc', 'GFR', 'hemo', 'al',\n",
    "               'SystolicBP', 'DiastolicBP', 'FastingBloodSugar', 'HbA1c',\n",
    "               'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL',\n",
    "               'CholesterolTriglycerides', 'ACR']\n",
    "\n",
    "data_dict = {\n",
    "    'df_336': {'df': df_336, 'features': features_m1, 'target': 'class'},\n",
    "    'df_857': {'df': df_857, 'features': features_m2, 'target': 'class'},\n",
    "    'df_ckd': {'df': df_ckd, 'features': features_m3, 'target': 'class'}\n",
    "}\n",
    "split_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af60669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Data Splitting and Scaling ---\n",
    "for name, data in data_dict.items():\n",
    "    df, features, target = data['df'], data['features'], data['target']\n",
    "    \n",
    "    # 3A: Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[features], df[target], test_size=0.2, random_state=42, stratify=df[target]\n",
    "    )\n",
    "    \n",
    "    # 3B: Handle Class Imbalance\n",
    "    if name == 'df_336':\n",
    "        cat_features_336 = ['htn', 'dm', 'cad', 'su', 'al'] \n",
    "        cat_indices = [X_train.columns.get_loc(col) for col in cat_features_336 if col in X_train.columns]\n",
    "        \n",
    "        class_counts = y_train.value_counts()\n",
    "        \n",
    "        minority_count = class_counts.min()\n",
    "        majority_count = class_counts.max()\n",
    "        current_ratio = minority_count / majority_count\n",
    "        \n",
    "        if current_ratio < 0.5:\n",
    "            target_ratio = 0.7  \n",
    "        else:\n",
    "            target_ratio = 'auto' \n",
    "        \n",
    "        smotenc = SMOTENC(\n",
    "            categorical_features=cat_indices, \n",
    "            sampling_strategy=target_ratio,\n",
    "            k_neighbors=min(5, minority_count - 1), \n",
    "            random_state=42\n",
    "        )\n",
    "        X_train, y_train = smotenc.fit_resample(X_train, y_train)\n",
    "                \n",
    "    elif name == 'df_857':\n",
    "        cat_features_857 = ['htn', 'dm', 'cad', 'su', 'al']\n",
    "        cat_indices = [X_train.columns.get_loc(col) for col in cat_features_857 if col in X_train.columns]\n",
    "        \n",
    "        class_counts = y_train.value_counts()\n",
    "        \n",
    "        minority_count = class_counts.min()\n",
    "        majority_count = class_counts.max()\n",
    "        current_ratio = minority_count / majority_count\n",
    "        \n",
    "        if current_ratio < 0.5:\n",
    "            target_ratio = 0.7\n",
    "        else:\n",
    "            target_ratio = 'auto'\n",
    "        \n",
    "        smotenc = SMOTENC(\n",
    "            categorical_features=cat_indices, \n",
    "            sampling_strategy=target_ratio,\n",
    "            k_neighbors=min(5, minority_count - 1),\n",
    "            random_state=42\n",
    "        )\n",
    "        X_train, y_train = smotenc.fit_resample(X_train, y_train)\n",
    "              \n",
    "    elif name == 'df_ckd':\n",
    "        class_counts = y_train.value_counts()\n",
    "        \n",
    "        minority_count = class_counts.min()\n",
    "        majority_count = class_counts.max()\n",
    "        current_ratio = minority_count / majority_count\n",
    "        \n",
    "        if current_ratio < 0.5:\n",
    "            target_ratio = 0.7\n",
    "        else:\n",
    "            target_ratio = 'auto'  \n",
    "        \n",
    "        smote = SMOTE(\n",
    "            sampling_strategy=target_ratio,\n",
    "            k_neighbors=min(5, minority_count - 1),\n",
    "            random_state=42\n",
    "        )\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "           \n",
    "    # Identify numerical columns for scaling\n",
    "    binary_cols = ['htn', 'dm', 'cad']\n",
    "    cols_to_scale = [col for col in X_train.columns if col not in binary_cols and X_train[col].dtype in [np.float64, np.int64]]\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled, X_test_scaled = X_train.copy(), X_test.copy()\n",
    "    X_train_scaled.loc[:, cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
    "    X_test_scaled.loc[:, cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "    \n",
    "    split_data[name] = {\n",
    "        'X_train': X_train_scaled, 'X_test': X_test_scaled, 'y_train': y_train, 'y_test': y_test, 'scaler': scaler\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e212f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Base Model Training with Imbalance Handling ---\n",
    "common_features = ['age', 'sc', 'hemo', 'dm', 'htn', 'al']\n",
    "X_train_ckd = split_data['df_ckd']['X_train']\n",
    "y_train_ckd = split_data['df_ckd']['y_train']\n",
    "X_test_ckd = split_data['df_ckd']['X_test']\n",
    "y_test_ckd = split_data['df_ckd']['y_test']\n",
    "\n",
    "# M1: SVC (Balanced, trained on D1 common features)\n",
    "svc_params = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "svc_grid = GridSearchCV(\n",
    "    SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "    svc_params, cv=5, scoring='balanced_accuracy',  # Changed from 'f1'\n",
    "    n_jobs=-1, verbose=0\n",
    ")\n",
    "svc_grid.fit(split_data['df_336']['X_train'][common_features], split_data['df_336']['y_train'])\n",
    "svc_optimized = svc_grid.best_estimator_\n",
    "\n",
    "# M2: Random Forest (Balanced, trained on D2 common features)\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    rf_params, cv=5, scoring='balanced_accuracy',  # Changed from 'f1'\n",
    "    n_jobs=-1, verbose=0\n",
    ")\n",
    "rf_grid.fit(split_data['df_857']['X_train'][common_features], split_data['df_857']['y_train'])\n",
    "rf_optimized = rf_grid.best_estimator_\n",
    "\n",
    "# M3: XG Boost (trained on D3 full features)\n",
    "scale_pos_weight_value = (y_train_ckd == 0).sum() / (y_train_ckd == 1).sum()\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'scale_pos_weight': [1, scale_pos_weight_value, scale_pos_weight_value * 2]  \n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    xgb_params, cv=5, scoring='balanced_accuracy', n_jobs=-1, verbose=0\n",
    ")\n",
    "xgb_grid.fit(X_train_ckd, y_train_ckd)\n",
    "xgb_optimized = xgb_grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5d07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Generate Meta-Features (Training and Test Sets) ---\n",
    "X_train_ckd_common = X_train_ckd[common_features]\n",
    "X_test_ckd_common = X_test_ckd[common_features]\n",
    "\n",
    "# Training Meta-Features\n",
    "m1_train_proba = svc_optimized.predict_proba(X_train_ckd_common)[:, 1]\n",
    "m2_train_proba = rf_optimized.predict_proba(X_train_ckd_common)[:, 1]\n",
    "m3_train_proba = xgb_optimized.predict_proba(X_train_ckd)[:, 1] \n",
    "X_meta_train = pd.DataFrame({'M1_Proba': m1_train_proba, 'M2_Proba': m2_train_proba, 'M3_Proba': m3_train_proba})\n",
    "\n",
    "# Test Meta-Features\n",
    "m1_test_proba = svc_optimized.predict_proba(X_test_ckd_common)[:, 1]\n",
    "m2_test_proba = rf_optimized.predict_proba(X_test_ckd_common)[:, 1]\n",
    "m3_test_proba = xgb_optimized.predict_proba(X_test_ckd)[:, 1]\n",
    "X_meta_test = pd.DataFrame({'M1_Proba': m1_test_proba, 'M2_Proba': m2_test_proba, 'M3_Proba': m3_test_proba})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679145f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cost_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Cost calculation\n",
    "    fn_cost = 1  # Missing a CKD patient\n",
    "    fp_cost = 2  # Wrongly diagnosing healthy person\n",
    "    \n",
    "    total_cost = (fn * fn_cost) + (fp * fp_cost)\n",
    "    \n",
    "    return -total_cost\n",
    "\n",
    "cost_scorer = make_scorer(custom_cost_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab513945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Meta-Learner Params: {'C': 0.1, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Meta-Learner Training (Level 1) ---\n",
    "meta_params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'class_weight': ['balanced', {0: 2, 1: 1}, {0: 3, 1: 1}, {0: 5, 1: 1}]\n",
    "}\n",
    "\n",
    "meta_grid = GridSearchCV(\n",
    "    LogisticRegression(solver='lbfgs', random_state=42),\n",
    "    meta_params,\n",
    "    cv=5,\n",
    "    scoring=cost_scorer, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "meta_grid.fit(X_meta_train, y_train_ckd)\n",
    "meta_learner_tuned_balanced = meta_grid.best_estimator_\n",
    "\n",
    "print(f\"Best Meta-Learner Params: {meta_grid.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b6efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Optimal Threshold ---\n",
    "y_pred_proba_final = meta_learner_tuned_balanced.predict_proba(X_meta_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test_ckd, y_pred_proba_final)\n",
    "youden_index = tpr - fpr\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds[optimal_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95df3476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved successfully!\n",
      "Optimal Threshold: 0.8377\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Save Models and Scalers ---\n",
    "models_dict = {\n",
    "    'svc': svc_optimized,\n",
    "    'rf': rf_optimized,\n",
    "    'xgb': xgb_optimized,\n",
    "    'meta': meta_learner_tuned_balanced,\n",
    "    'scaler_336': split_data['df_336']['scaler'],\n",
    "    'scaler_857': split_data['df_857']['scaler'],\n",
    "    'scaler_ckd': split_data['df_ckd']['scaler'],\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'common_features': common_features,\n",
    "    'features_m3': features_m3\n",
    "}\n",
    "\n",
    "with open('ckd_stacking_model.pkl', 'wb') as f:\n",
    "    pickle.dump(models_dict, f)\n",
    "\n",
    "print(\"Model trained and saved successfully!\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bbd85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
