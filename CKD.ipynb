{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24454705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve)\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22f96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "1. LOADING DATA...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Loading Dataset 1: Chronic Kidney Disease (UCI ID: 336)...\n"
     ]
    }
   ],
   "source": [
    "# 1. DATA LOADING\n",
    "print(\"-\"*80)\n",
    "print(\"1. LOADING DATA...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Fetch dataset\n",
    "print(\"\\nLoading Dataset 1: Chronic Kidney Disease (UCI ID: 336)...\")\n",
    "ckd_dataset1 = fetch_ucirepo(id=336)\n",
    "\n",
    "# Get features and targets\n",
    "x1 = ckd_dataset1.data.features.copy()\n",
    "y1 = ckd_dataset1.data.targets.copy()\n",
    "\n",
    "print(f\"UCI ID 336 - Dataset Shape: {x1.shape}\")\n",
    "print(f\"UCI ID 336 - Target Shape: {y1.shape}\")\n",
    "print(f\"UCI ID 336 - Features: {list(x1.columns)}\")\n",
    "print(f\"UCI ID 336 - Samples: {x1.shape[0]}\")\n",
    "print(f\"UCI ID 336 - Target Classes: {y1['class'].unique()}\")\n",
    "print(f\"UCI ID 336 - Target Classes Count: \\n{y1['class'].value_counts()}\")\n",
    "\n",
    "# Fetch dataset\n",
    "print(\"\\nLoading Dataset 2: Risk Factor Prediction of CKD (UCI ID: 857)...\")\n",
    "ckd_dataset1 = fetch_ucirepo(id=857)\n",
    "\n",
    "# Get features and targets\n",
    "x2 = ckd_dataset1.data.features.copy()\n",
    "y2 = ckd_dataset1.data.targets.copy()\n",
    "\n",
    "print(f\"UCI ID 857 - Dataset Shape: {x2.shape}\")\n",
    "print(f\"UCI ID 857 - Target Shape: {y2.shape}\")\n",
    "print(f\"UCI ID 857 - Features: {list(x2.columns)}\")\n",
    "print(f\"UCI ID 857 - Samples: {x2.shape[0]}\")\n",
    "print(f\"UCI ID 857 - Target Classes: {y2['class'].unique()}\")\n",
    "print(f\"UCI ID 857 - Target Classes Count: \\n{y2['class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1517dce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Classes: ['ckd' 'notckd']\n",
      "Target Classes Count: ckd       250\n",
      "notckd    150\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Replacing ckd\\t to ckd\n",
    "\n",
    "# Count problematic rows before cleaning\n",
    "ckd_tab_count = (y1['class'] == 'ckd\\t').sum()\n",
    "whitespace_issues = (y1['class'] != y1['class'].str.strip()).sum()\n",
    "\n",
    "# Clean the class column by stripping whitespace\n",
    "y1['class'] = y1['class'].str.strip()\n",
    "\n",
    "# Verify cleaning worked\n",
    "print(f\"Target Classes: {y1['class'].unique()}\")\n",
    "print(f\"Target Classes Count: {y1['class'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad855a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "2. DATA ANALYSIS & EXPLORATION...\n",
      "----------------------------------------\n",
      "\n",
      "--- Dataset 1 Analysis ---\n",
      "- Total samples: 400\n",
      "- Total features: 24\n",
      "- Target distribution:\n",
      "  • ckd: 250 (62.5%)\n",
      "  • notckd: 150 (37.5%)\n",
      "\n",
      "Missing Values Analysis:\n",
      "Feature  Missing_Count  Missing_Percent Data_Type\n",
      "    rbc            152            38.00    object\n",
      "   rbcc            131            32.75   float64\n",
      "   wbcc            106            26.50   float64\n",
      "    pot             88            22.00   float64\n",
      "    sod             87            21.75   float64\n",
      "    pcv             71            17.75   float64\n",
      "     pc             65            16.25    object\n",
      "   hemo             52            13.00   float64\n",
      "     su             49            12.25   float64\n",
      "     sg             47            11.75   float64\n",
      "     al             46            11.50   float64\n",
      "    bgr             44            11.00   float64\n",
      "     bu             19             4.75   float64\n",
      "     sc             17             4.25   float64\n",
      "     bp             12             3.00   float64\n",
      "    age              9             2.25   float64\n",
      "     ba              4             1.00    object\n",
      "    pcc              4             1.00    object\n",
      "    htn              2             0.50    object\n",
      "     dm              2             0.50    object\n",
      "    cad              2             0.50    object\n",
      "  appet              1             0.25    object\n",
      "     pe              1             0.25    object\n",
      "    ane              1             0.25    object\n",
      "\n",
      "Data Types Summary:\n",
      "- Numeric features (14): ['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wbcc', 'rbcc']\n",
      "- Categorical features (10): ['rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n",
      "\n",
      "--- Dataset 2 Analysis ---\n",
      "- Total samples: 200\n",
      "- Total features: 28\n",
      "- Target distribution:\n",
      "  • ckd: 128 (64.0%)\n",
      "  • notckd: 72 (36.0%)\n",
      "\n",
      "Missing Values Analysis:\n",
      "       Feature  Missing_Count  Missing_Percent Data_Type\n",
      "bp (Diastolic)              0              0.0     int64\n",
      "      bp limit              0              0.0     int64\n",
      "      affected              0              0.0     int64\n",
      "         stage              0              0.0    object\n",
      "           grf              0              0.0    object\n",
      "           ane              0              0.0     int64\n",
      "            pe              0              0.0     int64\n",
      "         appet              0              0.0     int64\n",
      "           cad              0              0.0     int64\n",
      "            dm              0              0.0     int64\n",
      "           htn              0              0.0     int64\n",
      "          wbcc              0              0.0    object\n",
      "          rbcc              0              0.0    object\n",
      "           pcv              0              0.0    object\n",
      "          hemo              0              0.0    object\n",
      "           pot              0              0.0    object\n",
      "            sc              0              0.0    object\n",
      "           sod              0              0.0    object\n",
      "            bu              0              0.0    object\n",
      "           bgr              0              0.0    object\n",
      "            ba              0              0.0     int64\n",
      "           pcc              0              0.0     int64\n",
      "            pc              0              0.0     int64\n",
      "            su              0              0.0    object\n",
      "           rbc              0              0.0     int64\n",
      "            al              0              0.0    object\n",
      "            sg              0              0.0    object\n",
      "           age              0              0.0    object\n",
      "\n",
      "Data Types Summary:\n",
      "- Numeric features (13): ['bp (Diastolic)', 'bp limit', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'affected']\n",
      "- Categorical features (15): ['sg', 'al', 'su', 'bgr', 'bu', 'sod', 'sc', 'pot', 'hemo', 'pcv', 'rbcc', 'wbcc', 'grf', 'stage', 'age']\n"
     ]
    }
   ],
   "source": [
    "# 2. DATA ANALYSIS & EXPLORATION\n",
    "print(\"-\"*80)\n",
    "print(\"2. DATA ANALYSIS & EXPLORATION...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Basic info dataset 1\n",
    "print(\"\\n--- Dataset 1 Analysis ---\")\n",
    "print(f\"- Total samples: {len(x1)}\")\n",
    "print(f\"- Total features: {x1.shape[1]}\")\n",
    "print(f\"- Target distribution:\")\n",
    "target_counts_336 = y1['class'].value_counts()\n",
    "for cls, count in target_counts_336.items():\n",
    "    print(f\"  • {cls}: {count} ({count/len(y1)*100:.1f}%)\")\n",
    "\n",
    "# Missing values analysis dataset 1\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_info_336 = []\n",
    "for col in x1.columns:\n",
    "    missing_count = x1[col].isnull().sum()\n",
    "    missing_percent = (missing_count / len(x1)) * 100\n",
    "    missing_info_336.append({\n",
    "        'Feature': col,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percent': missing_percent,\n",
    "        'Data_Type': str(x1[col].dtype)\n",
    "    })\n",
    "\n",
    "missing_df_336 = pd.DataFrame(missing_info_336)\n",
    "missing_df_336 = missing_df_336.sort_values('Missing_Percent', ascending=False)\n",
    "print(missing_df_336.to_string(index=False))\n",
    "\n",
    "# Data types analysis dataset 1\n",
    "print(f\"\\nData Types Summary:\")\n",
    "numeric_features_336 = []\n",
    "categorical_features_336 = []\n",
    "\n",
    "for col in x1.columns:\n",
    "    if x1[col].dtype in ['int64', 'float64']:\n",
    "        numeric_features_336.append(col)\n",
    "    else:\n",
    "        categorical_features_336.append(col)\n",
    "\n",
    "print(f\"- Numeric features ({len(numeric_features_336)}): {numeric_features_336}\")\n",
    "print(f\"- Categorical features ({len(categorical_features_336)}): {categorical_features_336}\")\n",
    "\n",
    "# Basic info dataset 2\n",
    "print(\"\\n--- Dataset 2 Analysis ---\")\n",
    "print(f\"- Total samples: {len(x2)}\")\n",
    "print(f\"- Total features: {x2.shape[1]}\")\n",
    "print(f\"- Target distribution:\")\n",
    "target_counts_857 = y2['class'].value_counts()\n",
    "for cls, count in target_counts_857.items():\n",
    "    print(f\"  • {cls}: {count} ({count/len(y2)*100:.1f}%)\")\n",
    "\n",
    "# Missing values analysis dataset 2\n",
    "print(\"\\nMissing Values Analysis:\")\n",
    "missing_info_857 = []\n",
    "for col in x2.columns:\n",
    "    missing_count = x2[col].isnull().sum()\n",
    "    missing_percent = (missing_count / len(x2)) * 100\n",
    "    missing_info_857.append({\n",
    "        'Feature': col,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percent': missing_percent,\n",
    "        'Data_Type': str(x2[col].dtype)\n",
    "    })\n",
    "\n",
    "missing_df_857 = pd.DataFrame(missing_info_857)\n",
    "missing_df_857 = missing_df_857.sort_values('Missing_Percent', ascending=False)\n",
    "print(missing_df_857.to_string(index=False))\n",
    "\n",
    "# Data types analysis dataset 2\n",
    "print(f\"\\nData Types Summary:\")\n",
    "numeric_features_857 = []\n",
    "categorical_features_857 = []\n",
    "\n",
    "for col in x2.columns:\n",
    "    if x2[col].dtype in ['int64', 'float64']:\n",
    "        numeric_features_857.append(col)\n",
    "    else:\n",
    "        categorical_features_857.append(col)\n",
    "\n",
    "print(f\"- Numeric features ({len(numeric_features_857)}): {numeric_features_857}\")\n",
    "print(f\"- Categorical features ({len(categorical_features_857)}): {categorical_features_857}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27c01ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "3. DATA CLEANING & PREPROCESSING...\n",
      "----------------------------------------\n",
      "\n",
      "Processing Dataset 1...\n",
      "\n",
      "Handling missing values for Dataset 1...\n",
      "Numeric columns for imputation: ['age', 'bp', 'sg', 'al', 'su', 'rbc', 'pc', 'pcc', 'ba', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo', 'pcv', 'wbcc', 'rbcc', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']\n",
      "Categorical columns for imputation: []\n",
      "\n",
      "Feature Engineering for Dataset 1...\n",
      "Missing values after imputation: 0\n",
      "Final feature set for dataset 1: 28 features\n",
      "\n",
      "Processing Dataset 2...\n",
      "\n",
      "Handling missing values for Dataset 2...\n",
      "Numeric columns for imputation: ['bp (diastolic)', 'bp limit', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'affected']\n",
      "Categorical columns for imputation: ['sg', 'al', 'su', 'bgr', 'bu', 'sod', 'sc', 'pot', 'hemo', 'pcv', 'rbcc', 'wbcc', 'grf', 'stage', 'age']\n",
      "Missing values after imputation: 0\n",
      "Final feature set: 29 features\n"
     ]
    }
   ],
   "source": [
    "# 3. DATA CLEANING & PREPROCESSING\n",
    "print(\"-\"*80)\n",
    "print(\"3. DATA CLEANING & PREPROCESSING...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "print(\"\\nProcessing Dataset 1...\")\n",
    "# Create a copy for processing\n",
    "x1_processed = x1.copy()\n",
    "y1_processed = y1.copy()\n",
    "\n",
    "y1_processed['class'] = y1_processed['class'].str.strip()\n",
    "\n",
    "# Binary mappings\n",
    "binary_mappings_336 = {\n",
    "    'rbc': {'normal': 0, 'abnormal': 1},\n",
    "    'pc': {'normal': 0, 'abnormal': 1},\n",
    "    'pcc': {'notpresent': 0, 'present': 1},\n",
    "    'ba': {'notpresent': 0, 'present': 1},\n",
    "    'htn': {'no': 0, 'yes': 1},\n",
    "    'dm': {'no': 0, 'yes': 1},\n",
    "    'cad': {'no': 0, 'yes': 1},\n",
    "    'appet': {'good': 0, 'poor': 1},\n",
    "    'pe': {'no': 0, 'yes': 1},\n",
    "    'ane': {'no': 0, 'yes': 1}\n",
    "}\n",
    "\n",
    "# Apply binary mappings\n",
    "for feature, mapping in binary_mappings_336.items():\n",
    "    if feature in x1_processed.columns:\n",
    "        x1_processed[feature] = x1_processed[feature].map(mapping)\n",
    "\n",
    "# Convert ordinal features to numeric\n",
    "for feature in ['sg', 'al', 'su']:\n",
    "    if feature in x1_processed.columns:\n",
    "        x1_processed[feature] = pd.to_numeric(x1_processed[feature], errors='coerce')\n",
    "\n",
    "# Convert target to binary\n",
    "y1_processed['class'] = y1_processed['class'].map({'notckd': 0, 'ckd': 1})\n",
    "\n",
    "print(\"\\nHandling missing values for Dataset 1...\")\n",
    "\n",
    "# Separate features by type for different imputation strategies\n",
    "numeric_cols_336 = x1_processed.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols_336 = x1_processed.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "print(f\"Numeric columns for imputation: {list(numeric_cols_336)}\")\n",
    "print(f\"Categorical columns for imputation: {list(categorical_cols_336)}\")\n",
    "\n",
    "# For numeric features, use KNN imputation\n",
    "if len(numeric_cols_336) > 0:\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    x1_processed[numeric_cols_336] = knn_imputer.fit_transform(x1_processed[numeric_cols_336])\n",
    "\n",
    "# For categorical features (if any remaining), use mode imputation\n",
    "if len(categorical_cols_336) > 0:\n",
    "    mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    x1_processed[categorical_cols_336] = mode_imputer.fit_transform(x1_processed[categorical_cols_336])\n",
    "\n",
    "# Feature engineering - create some additional features\n",
    "print(\"\\nFeature Engineering for Dataset 1...\")\n",
    "\n",
    "# BMI-like indicator using available blood parameters\n",
    "if all(col in x1_processed.columns for col in ['hemo', 'pcv']):\n",
    "    x1_processed['hemo_pcv_ratio'] = x1_processed['hemo'] / (x1_processed['pcv'] + 0.001)\n",
    "\n",
    "# Kidney function indicator\n",
    "if all(col in x1_processed.columns for col in ['sc', 'bu']):\n",
    "    x1_processed['kidney_function_score'] = x1_processed['sc'] * x1_processed['bu']\n",
    "\n",
    "# Blood pressure category\n",
    "if 'bp' in x1_processed.columns:\n",
    "    x1_processed['bp_category'] = pd.cut(x1_processed['bp'], \n",
    "                                       bins=[0, 90, 120, 140, 200], \n",
    "                                       labels=[0, 1, 2, 3])\n",
    "    x1_processed['bp_category'] = x1_processed['bp_category'].astype(int)\n",
    "    \n",
    "# Add dataset identifier\n",
    "x1_processed['dataset_source'] = 1\n",
    "\n",
    "print(f\"Missing values after imputation: {x1_processed.isnull().sum().sum()}\")\n",
    "print(f\"Final feature set for dataset 1: {x1_processed.shape[1]} features\")\n",
    "\n",
    "print(\"\\nProcessing Dataset 2...\")\n",
    "# Create a copy for processing\n",
    "x2_processed = x2.copy()\n",
    "y2_processed = y2.copy()\n",
    "\n",
    "x2_processed.columns = x2_processed.columns.str.lower()\n",
    "y2_processed.columns = y2_processed.columns.str.lower()\n",
    "\n",
    "print(\"\\nHandling missing values for Dataset 2...\")\n",
    "\n",
    "# Separate features by type for different imputation strategies\n",
    "numeric_cols_857 = x2_processed.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols_857 = x2_processed.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "print(f\"Numeric columns for imputation: {list(numeric_cols_857)}\")\n",
    "print(f\"Categorical columns for imputation: {list(categorical_cols_857)}\")\n",
    "\n",
    "# For numeric features, use KNN imputation\n",
    "if len(numeric_cols_857) > 0:\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    x2_processed[numeric_cols_857] = knn_imputer.fit_transform(x2_processed[numeric_cols_857])\n",
    "\n",
    "# For categorical features (if any remaining), use mode imputation\n",
    "if len(categorical_cols_857) > 0:\n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols_857:\n",
    "        le = LabelEncoder()\n",
    "        x2_processed[col] = le.fit_transform(x2_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "# Convert target to binary (0: no CKD, 1: CKD)\n",
    "if 'class' in y2_processed.columns:\n",
    "    unique_classes = y2_processed['class'].unique()\n",
    "    if len(unique_classes) == 2:\n",
    "        # Assuming the positive class contains 'ckd' or similar\n",
    "        y2_processed['class'] = y2_processed['class'].apply(\n",
    "            lambda x: 1 if str(x).lower() in ['ckd', '1', 'yes'] else 0\n",
    "        )\n",
    "\n",
    "# Add dataset identifier\n",
    "x2_processed['dataset_source'] = 2\n",
    "\n",
    "print(f\"Missing values after imputation: {x2_processed.isnull().sum().sum()}\")\n",
    "print(f\"Final feature set: {x2_processed.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4544960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "4. FEATURE ALIGNMENT & DATASET COMBINATION\n",
      "-----------------------------------\n",
      "\n",
      "Common features found: 23\n",
      "Features: ['age', 'pcv', 'pot', 'hemo', 'wbcc', 'al', 'su', 'bu', 'bgr', 'ba']...\n",
      "\n",
      "Combined dataset shape: (600, 24)\n",
      "Total samples: 600\n",
      "Total features: 24\n",
      "\n",
      "Combined target distribution:\n",
      "  - No CKD (0): 222 (37.00%)\n",
      "  - CKD (1): 378 (63.00%)\n"
     ]
    }
   ],
   "source": [
    "# 4. FEATURE ALIGNMENT & DATASET COMBINATION\n",
    "print(\"-\"*80)\n",
    "print(\"4. FEATURE ALIGNMENT & DATASET COMBINATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Find common features (excluding dataset_source)\n",
    "common_features = list(set(x1_processed.columns) & set(x2_processed.columns))\n",
    "common_features.remove('dataset_source')\n",
    "\n",
    "print(f\"\\nCommon features found: {len(common_features)}\")\n",
    "print(f\"Features: {common_features[:10]}...\" if len(common_features) > 10 else f\"Features: {common_features}\")\n",
    "\n",
    "# Create unified dataset with common features + dataset_source\n",
    "x1_common = x1_processed[common_features + ['dataset_source']].copy()\n",
    "x2_common = x2_processed[common_features + ['dataset_source']].copy()\n",
    "\n",
    "# Combine datasets\n",
    "x_combined = pd.concat([x1_common, x2_common], axis=0, ignore_index=True)\n",
    "y_combined = pd.concat([y1_processed['class'], y2_processed['class']], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {x_combined.shape}\")\n",
    "print(f\"Total samples: {len(x_combined)}\")\n",
    "print(f\"Total features: {x_combined.shape[1]}\")\n",
    "print(f\"\\nCombined target distribution:\")\n",
    "print(f\"  - No CKD (0): {(y_combined == 0).sum()} ({(y_combined == 0).sum()/len(y_combined)*100:.2f}%)\")\n",
    "print(f\"  - CKD (1): {(y_combined == 1).sum()} ({(y_combined == 1).sum()/len(y_combined)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff01f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "5. CREATING TRAIN/TEST SPLIT...\n",
      "-----------------------------------\n",
      "Training set: 480 samples\n",
      "Testing set: 120 samples\n",
      "Train class distribution: CKD=302, No CKD=178\n",
      "Test class distribution: CKD=76, No CKD=44\n"
     ]
    }
   ],
   "source": [
    "# 5. TRAIN/TEST SPLIT\n",
    "print(\"-\"*80)\n",
    "print(\"5. CREATING TRAIN/TEST SPLIT...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_combined, y_combined, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Train class distribution: CKD={np.sum(y_train)}, No CKD={len(y_train)-np.sum(y_train)}\")\n",
    "print(f\"Test class distribution: CKD={np.sum(y_test)}, No CKD={len(y_test)-np.sum(y_test)}\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3198c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "6. MODEL TRAINING & EVALUATION...\n",
      "--------------------------------------\n",
      "Training and evaluating models...\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Accuracy:    0.9833\n",
      "  Precision:   1.0000\n",
      "  Recall:      0.9737\n",
      "  F1-Score:    0.9867\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     0.9988\n",
      "  CV Score:    0.9458 ± 0.0267\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "  Accuracy:    1.0000\n",
      "  Precision:   1.0000\n",
      "  Recall:      1.0000\n",
      "  F1-Score:    1.0000\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     1.0000\n",
      "  CV Score:    0.9917 ± 0.0102\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Accuracy:    0.9917\n",
      "  Precision:   1.0000\n",
      "  Recall:      0.9868\n",
      "  F1-Score:    0.9934\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     1.0000\n",
      "  CV Score:    0.9750 ± 0.0182\n",
      "\n",
      "\n",
      "Training XGBoost...\n",
      "  Accuracy:    0.9833\n",
      "  Precision:   1.0000\n",
      "  Recall:      0.9737\n",
      "  F1-Score:    0.9867\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     0.9910\n",
      "  CV Score:    0.9833 ± 0.0169\n",
      "\n",
      "\n",
      "Training SVM (RBF)...\n",
      "  Accuracy:    0.6917\n",
      "  Precision:   0.7097\n",
      "  Recall:      0.8684\n",
      "  F1-Score:    0.7811\n",
      "  Specificity: 0.3864\n",
      "  ROC-AUC:     0.9181\n",
      "  CV Score:    0.6708 ± 0.0299\n",
      "\n",
      "\n",
      "Training SVM (Linear)...\n",
      "  Accuracy:    0.9917\n",
      "  Precision:   1.0000\n",
      "  Recall:      0.9868\n",
      "  F1-Score:    0.9934\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     0.9997\n",
      "  CV Score:    0.9583 ± 0.0228\n",
      "\n",
      "\n",
      "Training KNN...\n",
      "  Accuracy:    0.9750\n",
      "  Precision:   1.0000\n",
      "  Recall:      0.9605\n",
      "  F1-Score:    0.9799\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     0.9931\n",
      "  CV Score:    0.9396 ± 0.0312\n",
      "\n",
      "\n",
      "Training Naive Bayes...\n",
      "  Accuracy:    0.9417\n",
      "  Precision:   1.0000\n",
      "  Recall:      0.9079\n",
      "  F1-Score:    0.9517\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     0.9946\n",
      "  CV Score:    0.9187 ± 0.0241\n",
      "\n",
      "\n",
      "Training Decision Tree...\n",
      "  Accuracy:    0.9833\n",
      "  Precision:   0.9868\n",
      "  Recall:      0.9868\n",
      "  F1-Score:    0.9868\n",
      "  Specificity: 0.9773\n",
      "  ROC-AUC:     0.9821\n",
      "  CV Score:    0.9688 ± 0.0174\n",
      "\n",
      "\n",
      "Training AdaBoost...\n",
      "  Accuracy:    1.0000\n",
      "  Precision:   1.0000\n",
      "  Recall:      1.0000\n",
      "  F1-Score:    1.0000\n",
      "  Specificity: 1.0000\n",
      "  ROC-AUC:     1.0000\n",
      "  CV Score:    0.9854 ± 0.0106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. MODEL TRAINING & EVALUATION\n",
    "print(\"-\"*38)\n",
    "print(\"6. MODEL TRAINING & EVALUATION...\")\n",
    "print(\"-\"*38)\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBClassifier(random_state=42, n_estimators=100, eval_metric='logloss'),\n",
    "    'SVM (RBF)': SVC(random_state=42, probability=True, kernel='rbf', class_weight='balanced'),\n",
    "    'SVM (Linear)': SVC(random_state=42, probability=True, kernel='linear', class_weight='balanced'),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42, n_estimators=100)\n",
    "}\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "model_objects = {}\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Choose scaled or unscaled data based on model type\n",
    "    if name in ['Logistic Regression', 'SVM (RBF)', 'SVM (Linear)', 'KNN', 'Naive Bayes']:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    y_pred_proba = model.predict_proba(X_test_model)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC if probability predictions available\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_model, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Specificity (True Negative Rate)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Specificity': specificity,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'CV_Mean': cv_mean,\n",
    "        'CV_Std': cv_std\n",
    "    })\n",
    "    \n",
    "    # Store model object\n",
    "    model_objects[name] = model\n",
    "    \n",
    "    print(f\"  Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"  Precision:   {precision:.4f}\")\n",
    "    print(f\"  Recall:      {recall:.4f}\")\n",
    "    print(f\"  F1-Score:    {f1:.4f}\")\n",
    "    print(f\"  Specificity: {specificity:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"  ROC-AUC:     {roc_auc:.4f}\")\n",
    "    print(f\"  CV Score:    {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dfbd4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "7. COMPREHENSIVE MODEL PERFORMANCE COMPARISON\n",
      "--------------------------------------\n",
      "\n",
      "              Model  Accuracy  Precision  Recall  F1-Score  Specificity  ROC-AUC  CV_Mean  CV_Std\n",
      "      Random Forest    1.0000     1.0000  1.0000    1.0000       1.0000   1.0000   0.9917  0.0102\n",
      "           AdaBoost    1.0000     1.0000  1.0000    1.0000       1.0000   1.0000   0.9854  0.0106\n",
      "  Gradient Boosting    0.9917     1.0000  0.9868    0.9934       1.0000   1.0000   0.9750  0.0182\n",
      "       SVM (Linear)    0.9917     1.0000  0.9868    0.9934       1.0000   0.9997   0.9583  0.0228\n",
      "      Decision Tree    0.9833     0.9868  0.9868    0.9868       0.9773   0.9821   0.9688  0.0174\n",
      "Logistic Regression    0.9833     1.0000  0.9737    0.9867       1.0000   0.9988   0.9458  0.0267\n",
      "            XGBoost    0.9833     1.0000  0.9737    0.9867       1.0000   0.9910   0.9833  0.0169\n",
      "                KNN    0.9750     1.0000  0.9605    0.9799       1.0000   0.9931   0.9396  0.0312\n",
      "        Naive Bayes    0.9417     1.0000  0.9079    0.9517       1.0000   0.9946   0.9188  0.0241\n",
      "          SVM (RBF)    0.6917     0.7097  0.8684    0.7811       0.3864   0.9181   0.6708  0.0299\n",
      "\n",
      "================================================================================\n",
      "BEST PERFORMING MODELS\n",
      "================================================================================\n",
      "\n",
      "Best Accuracy:  Random Forest (1.0000)\n",
      "Best F1-Score:  Random Forest (1.0000)\n",
      "Best ROC-AUC:   Random Forest (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# 7. RESULTS SUMMARY\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"7. COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "print(\"\\n\" + results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Find best models\n",
    "best_accuracy = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "best_f1 = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "best_roc = results_df.loc[results_df['ROC-AUC'].idxmax()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMING MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Accuracy:  {best_accuracy['Model']} ({best_accuracy['Accuracy']:.4f})\")\n",
    "print(f\"Best F1-Score:  {best_f1['Model']} ({best_f1['F1-Score']:.4f})\")\n",
    "print(f\"Best ROC-AUC:   {best_roc['Model']} ({best_roc['ROC-AUC']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde0a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
